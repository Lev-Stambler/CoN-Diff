\documentclass[11pt]{article}
\usepackage{CSTheoryToolkitCMUStyle}
\usepackage{Custom}
\usepackage{cleveref}
% \usepackage{biblatex}
% \addbibresource{CSTheoryToolkitCMUStyle.bib}
\usepackage{mdframed}



%%%%% Stuff you can change %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\myname}{Lev Stambler}

%%%%% Section-renaming code by egreg
\makeatletter
% we use \prefix@<level> only if it is defined
\renewcommand{\@seccntformat}[1]{%
  \ifcsname prefix@#1\endcsname
    \csname prefix@#1\endcsname
  \else
    \csname the#1\endcsname\quad
  \fi}
% Now we define our homework section prefixes
\makeatother
%%%%%

\begin{document}

\title{CoN Diff: Verified Pseudo-Correlated Noise for Differential Privacy}

% \author{\myname}

\date{\today}
\maketitle

\input{sections/commands}

%\begin{abstract}
%\end{abstract}

\section{Introduction}
    ``Software is eating the world, but AI is going to eat software'' is a well-known quote by Jensen Huang, CEO of NVIDIA.
    Still, AI is notoriously data-hungry and privacy is a big concern.
    When considering private data---such as financial, medical, and political data---privacy is paramount.
    Unfortunately, privacy is often at odds with the utility of the data and the robustness to adversarial input of the AI model.
    In this work, we propose a novel approach to privacy-preserving machine learning, primarily building off of combining Differential Privacy DP) and Learning with Errors (LWE) as seen in Ref.~\cite{stevens2021efficientdifferentiallyprivatesecure}.
    We add zero-knowledge proofs (ZKPs) to ensure that inputted data is not poisoned with traitor-tracing capabilities.
    \footnote{Poisoned data is data that is intentionally manipulated to degrade the performance of the model.
For example, a model trying to calculate the average height of a group of people could be poisoned by inputting a height of 1000 feet.}
    
    Moreover, we use new cryptographic tools to allow for \emph{efficient and verifiable} repeated data releases without a repeated setup process.

\subsection{Current Techniques}
We can break down privacy-preserving machine learning into two main categories: cryptographic and non-cryptographic which usually refers to Multi-Party Compute (or Homomorphic Encryption) and Differential Privacy, respectively.

\subsubsection*{Differential Privacy}
For a more in-depth look into differential privacy, we recommend reading a recent survey on Local Differential Privacy (LDP)~\cite{yang2023local}.

DP uses noise to mask the contribution of individual data points in statistical analysis, ensuring that sensitive information is protected~\cite{dwork2006differential}. 
By adding random noise from distributions such as Laplace or Gaussian, DP limits the ability of an adversary to infer specific data, even when multiple queries are made. 
This balance between privacy and accuracy is often carefully calibrated with more noise enhancing privacy but reducing the utility of the results. 
In certain advanced scenarios, correlated noise, where noise is not independent but follows specific patterns, can improve privacy and utility, though verifying its correct implementation becomes more challenging.
\lev{shorten?}

There are a few different ways to formally describe differential privacy.
For simplicity, we will use the $(\eps, \delta)$-differential privacy definition, though our ideas can be extended to other definitions.
We have two flavors of differential privacy: local and non-local (or centralized) which is classical differential privacy.
%Intuitively, we can think of differential privacy as being a sort of ``2-wise'' independence condition on the input data: if we replace \emph{one} of the user's piece of data with some other possible piece of data, then differential privacy gaurentees that an adversary \emph{cannot} distinguish between the two datasets up to some probabality.
%Though not cryptographic in nature, differential privacy offers a notion of ``plausible-deniability'' which can also be generalized to a more $k$-wise independent setting \lev{TODO: cite?}.


%\lev{IDK if this is the actual defn}
\begin{definition}[$(\eps, \delta)$ Differential Privacy,~\cite{Bassily_2015}]
	Let $\eps > 0$ and $\calX$ be the domain of the data.
	Then, a randomized algorithm, $\calM : \calX^n \rightarrow \calM(\calX)$ which is applied to a dataset of $n$ elements satisfies $(\eps, \delta)$-differential privacy if and only if for all $\vec{x} \in \calX^n$ and $\vec{x}'$ where $\vec{x'}_i \neq \vec{x}_i$ for a specific $i$,
	\[
		\Pr[\calM(\vec{x}) \in S] \leq e^\eps \Pr[\calM(\vec{x}') \in S] + \delta
	\]
	for any possible $S \subseteq \text{Range}(\calM)$.
\end{definition}

Note that in the above, the randomized algorithm $\calM$ is applied globally.
We can also deffine differential privacy locally:

\begin{definition}[$(\eps, \delta)$-Local Differential Privacy,~\cite{Bassily_2015}]
	Let $\eps > 0$ and $\calX$ be the domain of the data.
	Then, a randomized algorithm, $\calM_{loc} : \calX \rightarrow \calM_{loc}(\calX)$ which is applied to the data independently satisfies $(\eps, \delta)$-local differential privacy if and only if for all pairs $x, x' \in \calX$
	for any possible $S \subseteq \text{Range}(\calM_{loc})$, we have
	\[
		\Pr[\calM_{loc}(x) \in S] \leq e^\eps \Pr[\calM_{loc}(x') \in S] + \delta.
	\]
\end{definition}

Non-local and local differential privacy are usually achieved by adding noise to a dataset.
Though many mechanisms exist, we will outline the \emph{Laplace Mechanism} for non-local differential privacy.

Let $f: \mathcal{X}^n \rightarrow \R^d$ be some query associated with the all of the user's data (e.g. learning the mean of the data).
Then, as outlined in \cite{Bassily_2015}, for any $x$,
\[
	\calM(\vec{x}) = f(\vec{x}) + N
\]
where $N \sim Lap(0, \Delta f/ \eps)^{d}$ where $\Delta f$ is the $\ell_1$ sensitivity of $f$.
Analogously, we can think of local-differential privacy as \[
	\calM(x) = f(x) + N.
\]

\textbf{Problem with Differential Privacy:} Non-local differential privacy requires trusting some centralized aggregator.
For example, in federated learning, each party would have to send there updated gradients \emph{in the clear} to an aggregator.
Recent work \cite{gupta2022recoveringprivatetextfederated} shows that an aggregator can almost perfectly recover the users' data from the gradient delta which could yeild a large privacy violation.
On the other hand, local differential privacy preserves users' privacy but the quality of the data degrades with an increase in the number of users.
Specifically, imagine trying to compute the sum of a dataset with $n$ users.
If each user adds Laplacian noise with a constant standard deviation, $\Delta$, then the sum of the data (calculated by adding together all of the noisy inputs from each user) has Laplacian noise with paramater $n \Delta$!

\subsubsection*{Cryptographic Approaches}
The other primary privacy preserving mechanism in machine learning is multi-party computation (MPC) \cite{yao1986generate}, which is known as as secure multi-party computation (SMPC) in machine learning to disambiguate from massive parallel computation.
We draw from a recent survey on SMPC~\cite{zhou2024secure}.
Given that our work is focused on differential privacy, we will not formally define SMPC here.
Rather, SMPC is a cryptographic protocol which allows multiple parties to compute a function on their private data without revealing their data to each other.
SMPC can also be integrated with other cryptogrpahic techniques (such as cut-and-choose type proofs, \cite{lindell2016fast}) to provide additional security guarantees: such as integrity checks on the data.

Unfortunately, SMPC is not a panacea.
Specifically, SMPC, especially with \emph{integrity checks on the input data}, is very slow and cannot scale to large datasets and hard computations.
Moreover, practical implementations require a lot of interaction between the parties, which can be a bottleneck in some applications \cite{zhao2019secure}.

\subsubsection*{Combining MPC, LWE, and Differential Privacy}
Ref.~\cite{stevens2021efficientdifferentiallyprivatesecure} introduced the idea of combining Learning with Error (LWE) with differential privacy for aggregation.
\footnote{Not too brag, but we discovered this idea independently prior to finding the referenced paper}
The key idea is to use an LWE sample for each party $i$, $b_i = A \cdot s_i + e_i$ where $s_i$ is secret and $e_i$ is private noise, to mask each users' data.
Then, using MPC or a similar protocol, the users can publish $s_{\sum} = \sum s_i$ which can then be used to compute an approximation of the aggregated one-time pads via $A \cdot s_{\sum} \approx \sum b_i = \sum A \cdot s_i + \sum e_i$.

While this is a very promising idea, it has a few (correctable!) drawbacks.
1) The setup process to generate $s_{\sum}$ must be repeated each time the party's release data.
2) This setup process can be expensive and expensive to verify (i.e. use verifable variants of MPC in the setup).

\section{Background and Proposed Solution}
We make extensive use of the following cryptographic primitives: \emph{Zero-knowledge proofs} (ZKPs)and \emph{homomorphic pseudo-random functions} (HPRFs).
We also use standard primitives such as commitments.

\emph{Zero-knowledge proofs} (ZKPs) are cryptographic protocols that allow one party, called the prover, to demonstrate the validity of a statement to another party, the verifier, without revealing any additional information beyond the fact that the statement is true. 
ZK-SNARKs (Zero-Knowledge Succinct Non-Interactive Argument of Knowledge) are a specific type of ZKP that are particularly efficient and scalable, making them suitable for a wide range of applications, from blockchain to secure messaging.
We will use RISC-0 to produce our ZKPs though other ZKPs could be used as well.
\footnote{\href{https://risczero.com/}{RISC-0 Homepage}}

\emph{homomorphic pseudo-random functions}, (HPRFs), introduced in Ref.~\cite{boneh2013key} are pseudo-random functions which are homomorphicover their keys under the addition operation.
Specifically, for keys $k_1, k_2$ and $x$, we have $F(k_1, x) + F(k_2, x) = F(k_1 + k_2, x)$ where $F$ is the HPRF.
Generally, HPRFs are constructed from Learning with Rounding (LWR) which is a variant of LWE and have been made more efficient in Refs.~\cite{newKeyHom, kim2020key}.


\subsubsection*{Proposed Solution}
We will specifically focus on improving privacy and integrity in decentralized machine learning where data is additively used as in Ref.~\cite{stevens2021efficientdifferentiallyprivatesecure}.
Instead of using LWE samples as one-time pads though, we will use homomorphic pseudo-random functions (HPRFs) \lev{TODO cite} which can in turn be constructed from LWE/ LWR.
\footnote{Specifically we require \emph{weak} HPRFs so that the keys are leakage resilient}
The key advantage is that we only need to do a verifiable setup process \emph{once} for each party and then the party can release data essentially an unlimited number of times (up to a polynomial/ sub-exponential number of times depending on assumptions).

\textbf{Adding in Verifiability:} Because each party's evaluation of the HPRF is relatively inexpensive and simple (via a matrix multiply and rounding), we can use ZK-SNARKs to verify that the party's published data is correctly masked relative to the HPRF, that the underlying data is within a certain range (too ensure that the data is not poisoned), and that the noise is not too large (also preventing poisoning).
As a nice benefit, adding verifiability gives a sort of ``traitor-tracing'' mechanism to the system for $N - 1$ parties while assuming that we have $N /2$ honest parties.
\footnote{Traitor tracing is specifically useful when using ``carrot and stick'' incentives to ensure that parties are honest (such as proof-of-stake).}

%Though not within scope, given the uniformity of each proof, a proof aggregator could also be used to enhance practicality.
We sketch the protocol in \cref{fig:prot}.

\begin{figure}
	\begin{mdframed}
		Setup (preferably ran within verifiable MPC, though not in scope) \begin{itemize}
			\item Each party $i$ generates a secret key $s_i$
			\item Using MPC, the parties compute $\sum s_i = s_{\sum}$
			\item Each party releases $Comm(s_i)$ where $Comm$ is a randomized commitment function
		\end{itemize}
		Aggregation for time step $t$ \begin{itemize}
			\item Each party $i$ calls $b_i = F(s_i, t)$  where $F$ is the HPRF
			\item Each party generates noise $\eta_i$ with standard deviation $O(1/n)$ and publishes data $ct_i = v_i + b_i + \eta_i$ as well as a proof that (1) $b_i$ is generated with key $s_i$ relative to $Comm(s_i)$ (2) $b_i = HPRF(s_i, t)$ and (3) $v_i + \eta_i$ are within some bounds specified by the protocol
			\item To decode the data, a third party checks all the proofs and does $\sum_i ct_i - HPRF(s_{\sum}, t) = \sum_i v_i + \sum \eta_i$
		\end{itemize}
	\end{mdframed}
	\caption{Outline of the Final Protocol}
	\label{fig:prot}
\end{figure}



%Introduced by Goldwasser, Micali, and Rackoff in the 1980s, ZKPs rely on complex mathematical principles to achieve this privacy-preserving verification. 
%ZKPs are particularly useful in applications where privacy is paramount, such as identity verification, blockchain transactions, and secure voting protocols.

%In cryptographic protocols, ZKPs are significant because they allow secure authentication and verification without compromising the confidentiality of sensitive information. 
%For example, in blockchain systems, ZKPs can verify transactions without revealing transaction details, ensuring both security and privacy in decentralized environments. 
%Additionally, ZKPs can be used in privacy-preserving computation, where multiple parties can collaboratively compute a function over their inputs without revealing those inputs to each other.
%This makes ZKPs a powerful tool in enhancing privacy and security across various cryptographic applications, from secure messaging to confidential financial transactions.

%Noise generation is crucial for privacy-preserving mechanisms like \emph{differential privacy} (DP) because it masks the contribution of individual data points in statistical analysis, ensuring that sensitive information is protected~\cite{dwork2006differential}. 
%By adding random noise from distributions such as Laplace or Gaussian, DP limits the ability of an adversary to infer specific data, even when multiple queries are made. 
%This balance between privacy and accuracy is often carefully calibrated with more noise enhancing privacy but reducing the utility of the results . 
%In certain advanced scenarios, correlated noise, where noise is not independent but follows specific patterns, can improve privacy and utility, though verifying its correct implementation becomes more challenging .
%
%In ZKPs, noise generation plays a complementary role by ensuring that sensitive data remains hidden while still allowing verification of computations.
%ZKPs allow a prover to demonstrate that noise has been correctly added according to privacy standards, without revealing the noise values or underlying data, preserving confidentiality. 
%This is particularly important in complex systems like federated learning (FL) or secure multi-party computation (MPC), where verification of proper noise addition across multiple parties is required for privacy guarantees. 
%In such systems, noise acts as both a tool for obfuscation and a means to provide proof validation, ensuring that privacy is upheld throughout the computation.
%
%\emph{Pseudo-correlated generators} (PCGs) are cryptographic tools designed to generate large amounts of correlated randomness from short, shared seeds between parties. 
%They are particularly useful in scenarios like secure multi-party computation (MPC), where secure, random correlations (such as oblivious transfer or vector oblivious linear evaluation) can reduce communication costs and improve computational efficiency. 
%Instead of having to continuously exchange randomness or communicate for every correlation instance, parties can precompute short, shared seeds and then expand these seeds locally into long pseudorandom strings, which emulate the desired correlated randomness. 
%This allows for efficient preprocessing phases in cryptographic protocols, which is crucial for scalability.
%
%In the context of data privacy, PCGs provide an efficient and secure way to introduce randomness needed for privacy-preserving operations without extensive communication overhead. 
%By allowing local expansion of correlated randomness, PCGs ensure that privacy guarantees, such as differential privacy, can be achieved efficiently even in complex protocols like MPC. 
%For example, in privacy-preserving machine learning or data analysis, PCGs can be used to generate the required randomness for adding noise to datasets, ensuring that individual data points are protected while enabling useful statistical analysis.

%\section{Background and Related Work}
%
%Noise is a fundamental component in many cryptographic primitives, serving to enhance security and privacy by obfuscating sensitive data. 
%In \emph{homomorphic encryption}~\cite{gentry2009fully}, noise is introduced during encryption to allow computations on ciphertexts without revealing the underlying plaintext. 
%While noise grows with each operation, it must be carefully managed to prevent decryption failure. 
%In DP, noise is added to statistical outputs to obscure individual contributions to a dataset, ensuring privacy even in the presence of multiple queries~\cite{dwork2006differential}. 
%Similarly, in \emph{Learning with Errors} (LWE)~\cite{regev2009lattices}, noise is central to the hardness assumption, where the challenge is to solve linear equations obscured by random noise, which provides post-quantum security.
%Each of these canonical works highlights how noise is not only a tool for privacy but also for achieving computational security in cryptographic systems.

%\subsubsection*{Differential Privacy}
%In this section, we draw heavily from a recent survey on Local Differential Privacy (LDP)~\cite{yang2023local}.
%
%There are a few different ways to formally describe differntial privacy.
%For simplicity, we will use the $(\eps, \delta)$-differential privacy definition, though our ideas can be extended to other definitions.
%
%Intuitively, we can think of differential privacy as being a sort of ``2-wise'' independence condition on the input data: if we replace \emph{one} of the user's piece of data with some other possible piece of data, then differential privacy gaurentees that an adversary \emph{cannot} distinguish between the two datasets up to some probabality.
%
%Though not cryptographic in nature, differential privacy offers a notion of ``plausible-deniability'' which can also be generalized to a more $k$-wise independent setting \lev{TODO: cite?}.
%
%We have two flavors of differential privacy: local and non-local (or centralized) which is classical differential privacy.
%
%
%\lev{IDK if this is the actual defn}
%\begin{definition}[$(\eps, \delta)$ Differential Privacy,~\cite{Bassily_2015}]
%	Let $\eps > 0$ and $\calX$ be the domain of the data.
%	Then, a randomized algorithm, $\calM : \calX^n \rightarrow \calM(\calX)$ which is applied to a dataset of $n$ elements satisfies $(\eps, \delta)$-differential privacy if and only if for all $\vec{x} \in \calX^n$ and $\vec{x}'$ where $\vec{x'}_i \neq \vec{x}_i$ for a specific $i$,
%	\[
%		\Pr[\calM(\vec{x}) \in S] \leq e^\eps \Pr[\calM(\vec{x}') \in S] + \delta
%	\]
%	for any possible $S \subseteq \text{Range}(\calM)$.
%\end{definition}
%
%Note that in the above, the randomized algorithm $\calM$ is applied globally.
%We can also deffine differential privacy locally:
%
%\begin{definition}[$(\eps, \delta)$-Local Differential Privacy,~\cite{Bassily_2015}]
%	Let $\eps > 0$ and $\calX$ be the domain of the data.
%	Then, a randomized algorithm, $\calM_{loc} : \calX \rightarrow \calM_{loc}(\calX)$ which is applied to the data independently satisfies $(\eps, \delta)$-local differential privacy if and only if for all pairs $x, x' \in \calX$
%	for any possible $S \subseteq \text{Range}(\calM_{loc})$, we have
%	\[
%		\Pr[\calM_{loc}(x) \in S] \leq e^\eps \Pr[\calM_{loc}(x') \in S] + \delta.
%	\]
%\end{definition}
%
%Non-local and local differential privacy are usually achieved by adding noise to a dataset.
%Though many mechanisms exist, we will outline the \emph{Laplace Mechanism} for non-local differential privacy.
%
%Let $f: \mathcal{X}^n \rightarrow \R^d$ be some query associated with the all of the user's data (e.g. learning the mean of the data).
%Then, as outlined in \cite{Bassily_2015}, for any $x$,
%\[
%	\calM(\vec{x}) = f(\vec{x}) + N
%\]
%where $N \sim Lap(0, \Delta f/ \eps)^{d}$ where $\Delta f$ is the $\ell_1$ sensitivity of $f$.
%Analogously, we can think of local-differential privacy as \[
%	\calM(x) = f(x) + N.
%\]
%
%\textbf{Problem with Differential Privacy:} Non-local differential privacy requires trusting some centralized aggregator.
%For example, in federated learning, each party would have to send there updated gradients \emph{in the clear} to an aggregator.
%Recent work \lev{TODO CITE} shows that an aggregator can almost perfectly recover the users' data from the gradient delta which could yeild a large privacy violation.
%On the other hand, local differential privacy preserves users' privacy but the quality of the data degrades with an increase in the number of users.
%Specifically, imagine trying to compute the sum of a dataset with $n$ users.
%If each user adds Laplacian noise with a constant standard deviation, $\Delta$, then the sum of the data (calculated by adding together all of the noisy inputs from each user) has Laplacian noise with paramater $n \Delta$!
%
%\subsubsection*{Cryptographic Approaches}
%The other primary privacy preserving mechanism in machine learning is multi-party computation (MPC \lev{CITE}), which is known as as secure multi-party computation (SMPC) in machine learning to disambiguate from massive parallel computation.
%We draw from a recent survey on SMPC~\cite{zhou2024secure}.
%Given that our work is focused on differential privacy, we will not formally define SMPC here.
%Rather, SMPC is a cryptographic protocol which allows multiple parties to compute a function on their private data without revealing their data to each other.
%SMPC can also be integrated with other cryptogrpahic techniques (such as cut-and-choose type proofs, \lev{CITE}) to provide additional security guarantees: such as integrity checks on the data.
%
%Unfortunately, SMPC is not a panacea.
%Specifically, SMPC, especially with \emph{integrity checks on the input data}, is very slow and cannot scale to large datasets and hard computations.
%Moreover, practical implementations require a lot of interaction between the parties, which can be a bottleneck in some applications \cite{zhao2019secure}.
%
%\subsubsection*{Combining MPC, LWE, and Differential Privacy}
%Ref.~\cite{stevens2021efficientdifferentiallyprivatesecure} introduced the idea of combining Learning with Error (LWE) with differential privacy for aggregation.
%\footnote{Not too brag, but we discovered this idea independently prior to finding the referenced paper}
%The key idea is to use an LWE sample for each party $i$, $b_i = A \cdot s_i + e_i$ where $s_i$ is secret and $e_i$ is private noise, to mask each users' data.
%Then, using MPC or a similar protocol, the users can publish $s_{\sum} = \sum s_i$ which can then be used to compute an approximation of the aggregated one-time pads via $A \cdot s_{\sum} \approx \sum b_i = \sum A \cdot s_i + \sum e_i$.
%
%While this is a very promising idea, it has a few (correctable!) drawbacks.
%1) The setup process to generate $s_{\sum}$ must be repeated each time the party's release data.
%2) This setup process can be expensive and expensive to verify (i.e. use verifable variants of MPC in the setup).
%

%\section{Proposed Solution}
%We will specifically focus on improving privacy and integrity in decentralized machine learning where data is additively used as in Ref.~\cite{stevens2021efficientdifferentiallyprivatesecure}.
%Instead of using LWE samples as one-time pads though, we will use homomorphic pseudo-random functions (HPRFs) \lev{TODO cite} which can in turn be consturcted from LWE.
%\footnote{Specifically we require \emph{weak} HPRFs so that the keys are leakage resilient}
%The key advantage is that we only need to do a verifiable setup process \emph{once} for each party and then the party can release data essentially an unlimited number of times (up to a polynomial/ sub-exponential number of times depending on assumptions).
%
%\textbf{Adding in Verifiability:} Because each pary's evaluation of the HPRF is relatively inexpensive and simple (via a matrix multiply and rounding), we can use ZK-SNARKs to verify that the party's published data is correctly masked relative to the HPRF, that the underlying data is within a certain range (too ensure that the data is not poisoned), and that the noise is not too large (also preventing poisoning).
%Indeed adding verifiability gives a sort of ``traitor-tracing'' mechanism to the system for $N - 1$ parties while assuming that we have $N /2$ honest parties.
%Though not within scope, given the uniformity of each proof, a proof aggregator could also be used to enhance practicality.
%For simplicity, we will use RISC-0 as our ZK-SNARK of choice \lev{TODO cite}.
%We sketch the protocol in \cref{fig:prot}.
%
%\begin{mdframed}
%	Setup (preferably ran within verifiable MPC, though not in scope) \begin{itemize}
%		\item Each party $i$ generates a secret key $s_i$
%		\item Using MPC, the parties compute $\sum s_i = s_{\sum}$
%		\item Each party releases $H(s_i)$ where $H$ is a hash function
%	\end{itemize}
%	Aggregation for time step $t$ \begin{itemize}
%		\item Each party $i$ calls $b_i = HPRF(s_i, t)$ 
%		\item Each party generates noise $\eta_i$ with standard deviation $O(1/n)$ and publishes data $ct_i = v_i + b_i + \eta_i$ as well as a proof that $b_i$ is generated with key $s_i$ relative to $H(s_i)$ and that $b_i = HPRF(s_i, t)$ and that $v_i + \eta_i$ are within some bounds
%		\item To decode the data, a third party checks all the proofs and does $\sum_i ct_i - HPRF(s_{\sum}, t) = \sum_i v_i + \sum \eta_i$
%	\end{itemize}
%	% \caption{Outline of the Final Protocol}
%	\label{fig:prot}
%\end{mdframed}

\bibliographystyle{alpha}
\bibliography{bib/ref}

\end{document}



